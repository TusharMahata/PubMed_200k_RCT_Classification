# PubMed_200k_RCT_Classification
**Title: PubMed 200k RCT Classification.**
**Exploring Sequential Sentence Classification in Medical Abstracts with PubMed 200k RCT**

**Introduction:**

The PubMed 200k RCT dataset, introduced in 2017, is a pivotal resource for sequential sentence classification in medical abstracts. With around 200,000 labeled Randomized Controlled Trial (RCT) abstracts, the dataset challenges researchers to decode the sequential roles of sentences in scientific literature.

This project aims to replicate the deep learning model from the original paper (https://arxiv.org/pdf/1612.05251.pdf), and modify it using advanced NLP techniques to classify sentences by their sequential significance. The goal is to contribute to automated literature analysis, providing insights into the structure and flow of medical abstracts.

I seek to reproduce the results of the PubMed 200k RCT paper and advance my understanding of how deep learning models can enhance comprehension of biomedical literature. This endeavor has the potential to impact information retrieval, summarization, and knowledge discovery in the medical domain.

**Dataset:**

The PubMed 200k RCT dataset, released in 2017, is a collection of around 200,000 labeled abstracts sourced from Randomized Controlled Trials (RCTs). It focuses on sequential sentence classification within medical abstracts, challenging researchers to discern the distinct roles of sentences. The dataset spans diverse medical topics and is widely used for training and evaluating natural language processing (NLP) models in biomedical research. Each abstract is annotated to indicate the sequential structure of sentences, making it a valuable resource for advancing automated literature analysis in the medical domain.

**Dataset Preprocessing:**

In this series of code cells, I've performed the following steps to prepare the PubMed 200k RCT dataset for a sequential sentence classification task:

1. Checked the contents of the dataset directory to ensure it includes the necessary files.
2. Unzipped the training data (`train.zip`) to access the abstracts.
3. Created functions to read and preprocess the abstracts, extracting information such as target labels, text of sentences, and line numbers.
4. Converted the abstracts into lists and performed one-hot encoding on the target labels using `OneHotEncoder`.
5. Used `LabelEncoder` to encode target labels into integers.
6. Obtained summary information, including the number of classes and class names.

These steps are crucial for setting up the dataset, encoding labels, and preparing the text data for training a deep learning model for sequential sentence classification.

**Model 0: Getting a baseline**

In this code snippet, I've implemented a basic text classification pipeline using scikit-learn:

1. **Pipeline Creation:**
   - Created a pipeline (`model_0`) with two stages:
  	- **TF-IDF Vectorization (`"tf-idf"`):** Converts text data into numerical vectors using the TF-IDF representation.
  	- **Multinomial Naive Bayes Classifier (`"clf"`):** A simple probabilistic classifier suitable for text classification tasks.

2. **Model Training:**
   - Used the pipeline to fit the training data (`train_sentences` and `train_labels_encoded`), allowing the model to learn the patterns in the text.

3. **Validation Evaluation:**
   - Assessed the model's performance on the validation dataset (`val_sentences` and `val_labels_encoded`) using the `score` method.

This baseline model provides a starting point for text classification, but further exploration with more advanced models or fine-tuning of hyperparameters may lead to improved performance.

**Preparing Data for Deep Sequence model**

The code prepares data for a deep sequence model using TensorFlow, focusing on a medical abstract dataset. Key steps include:

1. **Sentence Length Analysis:**
   - Calculates and analyzes the average sentence length in the training dataset.
   - Visualizes the distribution of sentence lengths using a histogram.

2. **Sequence Length Determination:**
   - Determines the sequence length that covers 95% of the sentence lengths, crucial for setting maximum sequence length during tokenization.

3. **Text Vectorization:**
   - Uses Keras' `TextVectorization` layer to convert text data into numerical vectors.
   - Adapts the vectorizer to the training dataset, specifying maximum tokens and output sequence length.

4. **Vocabulary Analysis:**
   - Analyzes the vocabulary generated by the text vectorizer, including vocabulary length and common/least common words.

5. **Token Embedding Layer:**
   - Defines an embedding layer using Keras' `Embedding` to convert tokenized words into dense vectors.

6. **TensorFlow Dataset Creation:**
   - Creates TensorFlow datasets for training, validation, and testing from tokenized and one-hot-encoded sentences and labels.
   - Batches and prefetches datasets for efficient model training.

This structured data preparation is essential for building and training deep learning models, particularly for tasks involving natural language processing on medical abstracts.


Model 1: Conv1D with token embeddings

**Model Architecture:**
The model processes input sentences through text vectorization and token embedding layers.
Incorporates a Conv1D layer with 64 filters and a kernel size of 5, followed by Global Average Pooling.
The output layer is a Dense layer with softmax activation for multiclass classification
**Model Compilation:**
Compiles the model with categorical crossentropy loss, Adam optimizer, and accuracy as the evaluation metric.
**Model Training:**
Fits the Conv1D model to the training dataset for three epochs.
The training history is stored for further analysis or visualization.

Model 2: Feature extraction with pretrained token embeddings
This code segment incorporates a pre-trained Universal Sentence Encoder (USE) from TensorFlow Hub into a feature extractor model for sequence classification. Here's a concise summary:

1. **Download Pretrained USE:**
   - Utilizes TensorFlow Hub to download the Universal Sentence Encoder (version 4).
   - Creates a non-trainable KerasLayer for the downloaded USE.

2. **Test Embedding on Random Sentence:**
   - Selects a random sentence from the training dataset and applies the USE embedding.
   - Prints a truncated version of the embedded sentence and its length.

3. **Define Feature Extractor Model:**
   - Constructs a feature extractor model using the USE layer.
   - Adds a Dense layer with ReLU activation on top of the USE layer.
   - Defines the output layer for multiclass classification.

4. **Compile and Fit the Model:**
   - Compiles the model with categorical crossentropy loss, Adam optimizer, and accuracy metric.
   - Fits the model to the training dataset for three epochs, considering dataset lengths for steps and validation.

This code demonstrates the integration of a powerful pre-trained embedding model (USE) into a TensorFlow feature extractor for sequence classification on a medical abstract dataset.

Model 3: Conv1D with character embeddings
The code segment performs the following tasks:

1. Defines a function `split_chars` to split sentences into individual characters.
2. Applies the character splitting function to sentences in the training, validation, and test datasets.
3. Computes the average character length of sentences and visualizes the distribution.
4. Creates a character-level token vectorizer using `TextVectorization` from TensorFlow and adapts it to the training characters.
5. Defines a character embedding layer using `Embedding` and tests it on a random training character sequence.
6. Constructs a Conv1D model for sequence classification with character embeddings, utilizing a Conv1D layer, GlobalMaxPooling, and a Dense output layer.
7. Compiles the Conv1D model with categorical crossentropy loss and Adam optimizer.
8. Creates character datasets and fits the model on the character-level data for three epochs.

Overall, the code focuses on preparing character-level data and training a Conv1D model for sequence classification, leveraging TensorFlow functionalities for text processing and embedding.

Model 4: Combining pretrained token embeddings + character embeddings (hybrid embedding layer)

The code segment creates a hybrid model for sequence classification by combining token-level and character-level information. Here's a summarized breakdown:

1. **Token Input Model:**
   - Uses the Universal Sentence Encoder to embed token-level input sentences.
   - Adds a Dense layer with ReLU activation on top.

2. **Character Input Model:**
   - Utilizes a character vectorizer and embedding layer for character-level input.
   - Incorporates a Bidirectional LSTM layer on character embeddings.

3. **Concatenation of Token and Character Outputs:**
   - Concatenates the output layers of the token and character models.

4. **Final Output Layers:**
   - Applies dropout layers and a Dense layer on top of the concatenated outputs.
   - Creates the final output layer with softmax activation.

5. **Combination of Token and Character Models:**
   - Constructs the final model that takes both token and character inputs.

6. **Compilation and Training:**
   - Compiles the model with categorical crossentropy loss and Adam optimizer.
   - Combines token and character data into a dataset, batches, and prefetches for efficient training.
   - Trains the model on the combined dataset for three epochs.

This code demonstrates the integration of token and character-level features into a unified model for enhanced sequence classification, aiming to capture both semantic and structural information in the input data.

Model 5: Transfer Learning with pretrained token embeddings + character embeddings + positional embeddings


The code defines a sophisticated model (model_5) for sequence classification by incorporating token-level, character-level, line number, and total lines information. Here's a summarized overview:

1. **Token Input Model:**
   - Embeds token sequences using the Universal Sentence Encoder.
   - Adds a Dense layer with ReLU activation.

2. **Character Input Model:**
   - Uses a character vectorizer and embedding layer for character-level input.
   - Applies a Bidirectional LSTM layer on character embeddings.

3. **Line Numbers and Total Lines Input Models:**
   - Processes one-hot encoded line numbers and total lines using Dense layers.

4. **Combine Token and Char Embeddings into a Hybrid Embedding:**
   - Concatenates the token and character model outputs.
   - Applies Dense and dropout layers.

5. **Combine Positional Embeddings with Token and Char Embeddings into a Tribrid Embedding:**
   - Concatenates line number, total lines, and the previous hybrid embedding.

6. **Output Layer:**
   - Creates the final output layer with softmax activation.

7. **Assemble the Model:**
   - Constructs the final model by specifying inputs and outputs.

8. **Compilation and Training:**
   - Compiles the model with categorical crossentropy loss, label smoothing, and Adam optimizer.
   - Combines line numbers, total lines, token, and character data into a dataset for training and validation.
   - Fits the model on the combined dataset for three epochs.

The model is designed to capture a diverse range of information, aiming to enhance sequence classification performance by considering different aspects of the input data.


